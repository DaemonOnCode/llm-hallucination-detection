# LLM Hallucination Detection

Large Language Models (LLMs) can occasionally produce false or misleading information, a phenomenon known as "hallucination." These hallucinations occur when the model generates content that is not grounded in its training data or real-world facts. Detecting hallucinations is crucial because they can lead to misinformation or incorrect conclusions, especially in critical applications like medical advice or legal analysis. Identifying hallucinations involves understanding the context in which the model operates, recognizing when the generated content deviates from known facts, and analyzing why the hallucination occurredâ€”often due to the model's attempt to fill in gaps or generate creative responses based on patterns rather than actual knowledge.

This repository contains the code for LLM Hallucination Detection.

## Instructions to run locally

### Backend

1. Navigate to the `backend` directory:
   ```bash
   cd backend
   ```
2. Install the required packages using the following command:

   ```bash
   pip install -r requirements.txt
   ```

3. Run the backend server using the following command:
   ```bash
   python3 app.py
   ```
4. The API will be hosted at `http://localhost:5000/`.

5. Finetuned model can be downloaded from [here](https://drive.google.com/drive/folders/1pVM51Q4rVALO0lei2yJcJEpPLMtn6ln2) and placed in the `backend/fine_tuned_model` directory.

### Frontend

1. Navigate to the `frontend` directory:
   ```bash
   cd frontend
   ```
2. Install the required packages using the following command:
   ```bash
   npm install
   ```
3. Run the frontend server using the following command:
   ```bash
   npm run dev
   ```
4. Open the browser and navigate to `http://localhost:3000/` to view the website.

## Requirements

- Backend

  - flask
  - pytorch
  - transformers
  - spacy
  - requests
  - openai
  - pypdf
  - docx
  - sentence-transformers
  - elasticsearch

  - sentence-piece
  - tqdm
  - pandas
  - numpy
  - matplotlib
  - seaborn
  - scikit-learn

- Frontend
  - Next.js
  - Tailwind CSS

## External Dependencies

You will need to manually install the following dependencies:

#### Ollama

- Instructions to install can be found [here](https://github.com/ollama/ollama/blob/main/README.md)

#### Elasticsearch

- Instructions to install can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html)

- Additional configuration which can be changed in Elasticsearch can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html)

## Website Screenshots

### Home Page

#### When user already has the text generated from a LLM

![Home Page - Generated](./assets/i2.png)

#### When user has to directly generate text from a LLM

![Home Page - To Generate](./assets/i1.png)

## Explanation

This project focuses on detecting hallucinations in outputs generated by Large Language Models (LLMs) by following these steps:

- Input Processing:
  A system was designed to accept LLM-generated statements for fact-checking. Users are provided with two options on the website: they can either input pre-generated text or generate outputs using their preferred LLM.

- Claim Extraction:
  A method was implemented to identify and extract factual claims from the input statements. This process involves generating triplets from the LLM output, which correspond to various claims that could be derived from the output.

- Information Retrieval:
  A component was developed to search for and retrieve relevant information from reliable sources, such as knowledge bases and trusted websites. Specifically, Wikidata and DBpedia were utilized for this purpose. Additionally, Elasticsearch was applied to any user-provided files where ground truth information might exist.

- Fact Verification and Classification:
  Algorithms were implemented to compare the extracted claims with the retrieved information. This was done in two ways:

  First, by assessing the semantic similarity between the ground truth and the LLM-generated answer.
  Next, by using a fine-tuned model (XLM-Roberta) to detect contradictions between the two texts.
  Based on the results, the LLM's output was classified as likely true, likely false, or unverifiable.

- Explanation Generation:
  Brief explanations or evidence supporting the classification were provided. Explanations were generated using an LLM by supplying the results from the previous steps as context.

- Finally, a website was developed to utilize the API, making the hallucination detection process accessible to users.

<h1 align="center"> Contributor </h1>
<table align="center">
<tr align="center">
<td>
<strong>Ansh Sharma</strong>
<p align="center">
<img src = "https://avatars.githubusercontent.com/u/60016461?v=4"  height="150" alt="Ansh Sharma">
</p>
<p align="center">
<a href = "https://github.com/DaemonOnCode"><img src = "https://www.iconninja.com/files/241/825/211/round-collaboration-social-github-code-circle-network-icon.svg" width="45" height = "45"/></a>
<a href = "https://www.linkedin.com/in/anshsharma09">
<img src = "https://www.iconninja.com/files/863/607/751/network-linkedin-social-connection-circular-circle-media-icon.svg" width="45" height="45"/>
</a>
</p>
</td>
</tr>
</table>

# Citation

## Please cite the repo if you use the data or code in this repo.

```
@misc{LLM Hallucination Detection,
  author = {Ansh Sharma},
  title = {LLM Hallucination Detection},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/DaemonOnCode/llm-hallucination-detection},
}
```
